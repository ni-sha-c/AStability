{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Language Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "This is a tutorial on training a sequence-to-sequence model that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>`__ module.\n",
    "\n",
    "The PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You Need <https://arxiv.org/pdf/1706.03762.pdf>`\n",
    "\n",
    "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
    "to be superior in quality for many sequence-to-sequence tasks while being more\n",
    "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
    "mechanism (implemented as `nn.MultiheadAttention <https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>`\n",
    "to draw global dependencies between input and output. The ``nn.Transformer``\n",
    "module is highly modularized such that a single component (e.g.,\n",
    "`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>` can be easily adapted/composed.\n",
    "\n",
    ".. image:: ../_static/img/transformer_architecture.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__.\n",
    "Along with the input sequence, a square attention mask is required because the\n",
    "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
    "the earlier positions in the sequence. For the language modeling task, any\n",
    "tokens on the future positions should be masked. To produce a probability\n",
    "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
    "model is passed through a linear layer followed by a log-softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "from typing import Tuple\n",
    "\n",
    "# import copy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# This tutorial uses ``torchtext`` to generate Wikitext-2 dataset.\n",
    "# To access torchtext datasets, please install torchdata following instructions at https://github.com/pytorch/data. \n",
    "#\n",
    "# UPDATE: using Wikitext-103, see https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/\n",
    "# The vocab object is built based on the train dataset and is used to numericalize\n",
    "# tokens into tensors. Wikitext-2 represents rare tokens as `<unk>`.\n",
    "#\n",
    "# Given a 1-D vector of sequential data, ``batchify()`` arranges the data\n",
    "# into ``batch_size`` columns. If the data does not divide evenly into\n",
    "# ``batch_size`` columns, then the data is trimmed to fit. For instance, with\n",
    "# the alphabet as the data (total length of 26) and ``batch_size=4``, we would\n",
    "# divide the alphabet into 4 sequences of length 6:\n",
    "#\n",
    "# .. math::\n",
    "#   \\begin{bmatrix}\n",
    "#   \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "#   \\end{bmatrix}\n",
    "#   \\Rightarrow\n",
    "#   \\begin{bmatrix}\n",
    "#   \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "#   \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "#   \\end{bmatrix}\n",
    "#\n",
    "# Batching enables more parallelizable processing. However, batching means that\n",
    "# the model treats each column independently; for example, the dependence of\n",
    "# ``G`` and ``F`` can not be learned in the example above.\n",
    "#\n",
    "\n",
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# train_iter = WikiText2(split='train')\n",
    "train_iter = WikiText103(split='train')\n",
    "tokenizer  = get_tokenizer('basic_english')\n",
    "vocab      = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>']) \n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText103()\n",
    "train_data_orig = data_process(train_iter)\n",
    "val_data_orig   = data_process(val_iter)\n",
    "test_data_orig  = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1015443]),\n",
       " torch.Size([101544324]),\n",
       " torch.Size([214572]),\n",
       " torch.Size([242042]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prc = 0.01\n",
    "train_data_sel = train_data_orig[:np.round(train_data_orig.shape[0]*prc).astype(np.int)]\n",
    "train_data_sel.shape, train_data_orig.shape, val_data_orig.shape, test_data_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "eval_batch_size = 100\n",
    "train_data = batchify(train_data_sel, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data   = batchify(val_data_orig, eval_batch_size)\n",
    "test_data  = batchify(test_data_orig, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10154, 100]), torch.Size([2145, 100]), torch.Size([2420, 100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate input and target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ``get_batch()`` generates a pair of input-target sequences for\n",
    "# the transformer model. It subdivides the source data into chunks of\n",
    "# length ``bptt``. For the language modeling task, the model needs the\n",
    "# following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "# weâ€™d get the following two Variables for ``i`` = 0:\n",
    "#\n",
    "# .. image:: ../_static/img/transformer_input_target.png\n",
    "#\n",
    "# It should be noted that the chunks are along dimension 0, consistent\n",
    "# with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "# ``N`` is along dimension 1.\n",
    "#\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    log_interval = 1000 # num_batches\n",
    "    losses = []\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n",
    "    return losses\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# The model hyperparameters are defined below. The vocab size is\n",
    "# equal to the length of the vocab object.\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize  = 300  # embedding dimension\n",
    "d_hid   = 300  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 3  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nheads  = 3  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0  # dropout probability\n",
    "model   = TransformerModel(ntokens, emsize, nheads, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gstore/scratch/u/loukasa1/alg-stability/model_weights/wikitext103-transformer-prc:0.01_29.04.2022-12:01:24\n",
      "   1 | time: 57.21s | train loss  9.91 | valid loss  8.33 | valid ppl  4130.84\n",
      "   2 | time: 57.37s | train loss  7.74 | valid loss  7.27 | valid ppl  1442.79\n",
      "   3 | time: 57.44s | train loss  6.95 | valid loss  6.80 | valid ppl   894.83\n",
      "   4 | time: 57.46s | train loss  6.46 | valid loss  6.61 | valid ppl   740.15\n",
      "   5 | time: 57.45s | train loss  6.09 | valid loss  6.50 | valid ppl   662.09\n",
      "   6 | time: 57.51s | train loss  5.76 | valid loss  6.41 | valid ppl   605.33\n",
      "   7 | time: 57.51s | train loss  5.47 | valid loss  6.41 | valid ppl   607.15\n",
      "   8 | time: 57.54s | train loss  5.19 | valid loss  6.43 | valid ppl   621.61\n",
      "   9 | time: 57.57s | train loss  4.92 | valid loss  6.55 | valid ppl   696.98\n",
      "  10 | time: 57.58s | train loss  4.66 | valid loss  6.64 | valid ppl   766.46\n",
      "  11 | time: 57.61s | train loss  4.44 | valid loss  6.84 | valid ppl   932.34\n",
      "  12 | time: 57.65s | train loss  4.24 | valid loss  7.00 | valid ppl  1096.48\n",
      "  13 | time: 57.66s | train loss  4.05 | valid loss  7.18 | valid ppl  1308.08\n",
      "  14 | time: 57.63s | train loss  3.87 | valid loss  7.35 | valid ppl  1558.99\n",
      "  15 | time: 57.66s | train loss  3.70 | valid loss  7.56 | valid ppl  1911.49\n",
      "  16 | time: 57.68s | train loss  3.54 | valid loss  7.65 | valid ppl  2090.52\n",
      "  17 | time: 57.70s | train loss  3.40 | valid loss  7.91 | valid ppl  2727.50\n",
      "  18 | time: 57.68s | train loss  3.26 | valid loss  8.05 | valid ppl  3124.14\n",
      "  19 | time: 57.76s | train loss  3.13 | valid loss  8.22 | valid ppl  3702.90\n",
      "  20 | time: 57.71s | train loss  3.01 | valid loss  8.41 | valid ppl  4488.56\n",
      "  21 | time: 57.80s | train loss  2.89 | valid loss  8.57 | valid ppl  5284.08\n",
      "  22 | time: 57.74s | train loss  2.77 | valid loss  8.66 | valid ppl  5782.68\n",
      "  23 | time: 57.73s | train loss  2.67 | valid loss  8.91 | valid ppl  7439.46\n",
      "  24 | time: 57.75s | train loss  2.57 | valid loss  9.12 | valid ppl  9157.75\n",
      "  25 | time: 57.73s | train loss  2.47 | valid loss  9.22 | valid ppl 10102.92\n",
      "  26 | time: 57.76s | train loss  2.38 | valid loss  9.40 | valid ppl 12109.22\n",
      "  27 | time: 57.79s | train loss  2.29 | valid loss  9.53 | valid ppl 13762.56\n",
      "  28 | time: 57.79s | train loss  2.21 | valid loss  9.77 | valid ppl 17466.37\n",
      "  29 | time: 57.80s | train loss  2.13 | valid loss  9.77 | valid ppl 17533.25\n",
      "  30 | time: 57.79s | train loss  2.06 | valid loss  9.98 | valid ppl 21526.07\n",
      "  31 | time: 57.79s | train loss  1.98 | valid loss 10.17 | valid ppl 26152.88\n",
      "  32 | time: 57.80s | train loss  1.92 | valid loss 10.25 | valid ppl 28410.29\n",
      "  33 | time: 57.74s | train loss  1.86 | valid loss 10.29 | valid ppl 29339.46\n",
      "  34 | time: 57.82s | train loss  1.80 | valid loss 10.44 | valid ppl 34228.55\n",
      "  35 | time: 57.79s | train loss  1.74 | valid loss 10.59 | valid ppl 39933.69\n",
      "  36 | time: 57.80s | train loss  1.69 | valid loss 10.67 | valid ppl 43077.55\n",
      "  37 | time: 57.81s | train loss  1.64 | valid loss 10.85 | valid ppl 51677.50\n",
      "  38 | time: 57.80s | train loss  1.59 | valid loss 11.02 | valid ppl 61228.82\n",
      "  39 | time: 57.82s | train loss  1.54 | valid loss 11.16 | valid ppl 70273.60\n",
      "  40 | time: 57.82s | train loss  1.50 | valid loss 11.17 | valid ppl 70799.64\n",
      "  41 | time: 57.81s | train loss  1.46 | valid loss 11.37 | valid ppl 86304.89\n",
      "  42 | time: 57.76s | train loss  1.42 | valid loss 11.47 | valid ppl 95556.91\n",
      "  43 | time: 57.80s | train loss  1.39 | valid loss 11.53 | valid ppl 102084.93\n",
      "  44 | time: 57.82s | train loss  1.36 | valid loss 11.74 | valid ppl 125823.09\n",
      "  45 | time: 57.81s | train loss  1.32 | valid loss 11.81 | valid ppl 134473.82\n",
      "  46 | time: 57.85s | train loss  1.29 | valid loss 11.89 | valid ppl 145970.85\n",
      "  47 | time: 57.80s | train loss  1.26 | valid loss 11.98 | valid ppl 159347.41\n",
      "  48 | time: 57.78s | train loss  1.24 | valid loss 12.10 | valid ppl 180204.14\n",
      "  49 | time: 57.80s | train loss  1.21 | valid loss 12.16 | valid ppl 190885.83\n",
      "  50 | time: 57.80s | train loss  1.18 | valid loss 12.24 | valid ppl 207360.47\n",
      "  51 | time: 57.81s | train loss  1.16 | valid loss 12.42 | valid ppl 247006.35\n",
      "  52 | time: 57.79s | train loss  1.14 | valid loss 12.40 | valid ppl 243431.13\n",
      "  53 | time: 57.78s | train loss  1.12 | valid loss 12.46 | valid ppl 257093.59\n",
      "  54 | time: 57.83s | train loss  1.10 | valid loss 12.51 | valid ppl 271883.97\n",
      "  55 | time: 57.82s | train loss  1.08 | valid loss 12.59 | valid ppl 293260.22\n",
      "  56 | time: 57.82s | train loss  1.06 | valid loss 12.68 | valid ppl 322257.96\n",
      "  57 | time: 57.78s | train loss  1.04 | valid loss 12.87 | valid ppl 390290.35\n",
      "  58 | time: 57.79s | train loss  1.03 | valid loss 12.86 | valid ppl 382981.84\n",
      "  59 | time: 57.82s | train loss  1.01 | valid loss 13.01 | valid ppl 446831.23\n",
      "  60 | time: 57.81s | train loss  1.00 | valid loss 12.98 | valid ppl 432826.59\n",
      "  61 | time: 57.78s | train loss  0.98 | valid loss 13.08 | valid ppl 478324.10\n",
      "  62 | time: 57.80s | train loss  0.96 | valid loss 13.03 | valid ppl 457992.03\n",
      "  63 | time: 57.81s | train loss  0.95 | valid loss 13.19 | valid ppl 532339.07\n",
      "  64 | time: 57.83s | train loss  0.94 | valid loss 13.32 | valid ppl 609532.48\n",
      "  65 | time: 57.78s | train loss  0.92 | valid loss 13.35 | valid ppl 630731.75\n",
      "  66 | time: 57.79s | train loss  0.91 | valid loss 13.41 | valid ppl 666231.67\n",
      "  67 | time: 57.79s | train loss  0.90 | valid loss 13.44 | valid ppl 688235.67\n",
      "  68 | time: 57.82s | train loss  0.89 | valid loss 13.56 | valid ppl 775620.99\n",
      "  69 | time: 57.80s | train loss  0.88 | valid loss 13.71 | valid ppl 900924.13\n",
      "  70 | time: 57.85s | train loss  0.87 | valid loss 13.57 | valid ppl 782927.57\n",
      "  71 | time: 57.82s | train loss  0.86 | valid loss 13.76 | valid ppl 946512.99\n",
      "  72 | time: 57.80s | train loss  0.85 | valid loss 13.77 | valid ppl 955746.85\n",
      "  73 | time: 57.84s | train loss  0.84 | valid loss 13.80 | valid ppl 985286.73\n",
      "  74 | time: 57.81s | train loss  0.83 | valid loss 13.83 | valid ppl 1018597.16\n",
      "  75 | time: 57.80s | train loss  0.82 | valid loss 13.74 | valid ppl 927940.72\n",
      "  76 | time: 57.77s | train loss  0.81 | valid loss 13.98 | valid ppl 1176164.46\n",
      "  77 | time: 57.78s | train loss  0.80 | valid loss 13.96 | valid ppl 1160523.43\n",
      "  78 | time: 57.80s | train loss  0.79 | valid loss 14.03 | valid ppl 1237476.68\n",
      "  79 | time: 57.80s | train loss  0.79 | valid loss 13.95 | valid ppl 1144357.12\n",
      "  80 | time: 57.82s | train loss  0.78 | valid loss 14.11 | valid ppl 1338910.28\n",
      "  81 | time: 57.80s | train loss  0.77 | valid loss 14.11 | valid ppl 1342159.71\n",
      "  82 | time: 57.80s | train loss  0.76 | valid loss 14.13 | valid ppl 1374668.64\n",
      "  83 | time: 57.78s | train loss  0.76 | valid loss 14.26 | valid ppl 1559547.34\n",
      "  84 | time: 57.83s | train loss  0.75 | valid loss 14.26 | valid ppl 1555187.59\n",
      "  85 | time: 57.80s | train loss  0.74 | valid loss 14.29 | valid ppl 1609194.94\n",
      "  86 | time: 57.79s | train loss  0.74 | valid loss 14.20 | valid ppl 1469664.34\n",
      "  87 | time: 57.80s | train loss  0.73 | valid loss 14.51 | valid ppl 2003578.93\n",
      "  88 | time: 57.82s | train loss  0.72 | valid loss 14.48 | valid ppl 1941906.13\n",
      "  89 | time: 57.82s | train loss  0.72 | valid loss 14.40 | valid ppl 1790622.21\n",
      "  90 | time: 57.79s | train loss  0.71 | valid loss 14.56 | valid ppl 2105494.86\n",
      "  91 | time: 57.80s | train loss  0.71 | valid loss 14.51 | valid ppl 1993786.93\n",
      "  92 | time: 57.80s | train loss  0.70 | valid loss 14.69 | valid ppl 2388655.37\n",
      "  93 | time: 57.79s | train loss  0.69 | valid loss 14.41 | valid ppl 1805733.64\n",
      "  94 | time: 57.77s | train loss  0.69 | valid loss 14.67 | valid ppl 2353165.05\n",
      "  95 | time: 57.82s | train loss  0.69 | valid loss 14.67 | valid ppl 2345764.77\n",
      "  96 | time: 57.81s | train loss  0.68 | valid loss 14.44 | valid ppl 1865727.38\n",
      "  97 | time: 57.78s | train loss  0.67 | valid loss 14.73 | valid ppl 2505002.87\n",
      "  98 | time: 57.83s | train loss  0.67 | valid loss 14.72 | valid ppl 2466069.25\n",
      "  99 | time: 57.84s | train loss  0.67 | valid loss 14.61 | valid ppl 2218450.66\n",
      " 100 | time: 57.82s | train loss  0.66 | valid loss 14.67 | valid ppl 2355673.50\n",
      " 101 | time: 57.81s | train loss  0.65 | valid loss 14.75 | valid ppl 2550714.98\n",
      " 102 | time: 57.86s | train loss  0.65 | valid loss 14.82 | valid ppl 2742689.61\n",
      " 103 | time: 57.84s | train loss  0.65 | valid loss 14.81 | valid ppl 2694534.85\n",
      " 104 | time: 57.82s | train loss  0.64 | valid loss 14.83 | valid ppl 2758529.50\n",
      " 105 | time: 57.83s | train loss  0.64 | valid loss 15.00 | valid ppl 3281466.05\n",
      " 106 | time: 57.82s | train loss  0.64 | valid loss 14.82 | valid ppl 2741529.25\n",
      " 107 | time: 57.85s | train loss  0.63 | valid loss 14.88 | valid ppl 2885558.62\n",
      " 108 | time: 57.86s | train loss  0.63 | valid loss 15.03 | valid ppl 3385082.61\n",
      " 109 | time: 57.83s | train loss  0.63 | valid loss 14.92 | valid ppl 3007905.48\n",
      " 110 | time: 57.84s | train loss  0.62 | valid loss 14.86 | valid ppl 2853297.78\n",
      " 111 | time: 57.84s | train loss  0.61 | valid loss 15.11 | valid ppl 3654365.81\n",
      " 112 | time: 57.81s | train loss  0.61 | valid loss 15.15 | valid ppl 3785179.15\n",
      " 113 | time: 57.85s | train loss  0.61 | valid loss 15.16 | valid ppl 3832670.28\n",
      " 114 | time: 57.83s | train loss  0.61 | valid loss 15.08 | valid ppl 3536162.11\n",
      " 115 | time: 57.84s | train loss  0.60 | valid loss 15.09 | valid ppl 3594444.16\n",
      " 116 | time: 57.84s | train loss  0.60 | valid loss 15.24 | valid ppl 4162013.43\n",
      " 117 | time: 57.85s | train loss  0.60 | valid loss 15.19 | valid ppl 3947745.95\n",
      " 118 | time: 57.84s | train loss  0.60 | valid loss 15.15 | valid ppl 3800251.91\n",
      " 119 | time: 57.87s | train loss  0.59 | valid loss 15.20 | valid ppl 4005153.43\n",
      " 120 | time: 57.84s | train loss  0.59 | valid loss 15.17 | valid ppl 3888608.65\n",
      " 121 | time: 57.83s | train loss  0.58 | valid loss 15.35 | valid ppl 4619178.53\n",
      " 122 | time: 57.85s | train loss  0.58 | valid loss 15.22 | valid ppl 4061524.39\n",
      " 123 | time: 57.86s | train loss  0.58 | valid loss 15.35 | valid ppl 4657052.72\n",
      " 124 | time: 57.84s | train loss  0.57 | valid loss 15.06 | valid ppl 3468080.35\n",
      " 125 | time: 57.86s | train loss  0.57 | valid loss 15.48 | valid ppl 5256842.56\n",
      " 126 | time: 57.86s | train loss  0.57 | valid loss 15.26 | valid ppl 4258263.54\n",
      " 127 | time: 57.82s | train loss  0.57 | valid loss 15.46 | valid ppl 5154729.33\n",
      " 128 | time: 57.81s | train loss  0.56 | valid loss 15.35 | valid ppl 4643617.46\n",
      " 129 | time: 57.85s | train loss  0.56 | valid loss 15.46 | valid ppl 5202915.60\n",
      " 130 | time: 57.82s | train loss  0.56 | valid loss 15.56 | valid ppl 5696838.50\n",
      " 131 | time: 57.85s | train loss  0.56 | valid loss 15.42 | valid ppl 4976842.32\n",
      " 132 | time: 57.85s | train loss  0.55 | valid loss 15.48 | valid ppl 5283354.04\n",
      " 133 | time: 57.83s | train loss  0.55 | valid loss 15.47 | valid ppl 5238158.13\n",
      " 134 | time: 57.81s | train loss  0.55 | valid loss 15.50 | valid ppl 5411327.96\n",
      " 135 | time: 57.81s | train loss  0.55 | valid loss 15.68 | valid ppl 6421035.87\n",
      " 136 | time: 57.81s | train loss  0.54 | valid loss 15.54 | valid ppl 5616439.25\n",
      " 137 | time: 57.85s | train loss  0.54 | valid loss 15.72 | valid ppl 6707855.05\n",
      " 138 | time: 57.82s | train loss  0.54 | valid loss 15.61 | valid ppl 5987050.46\n",
      " 139 | time: 57.84s | train loss  0.54 | valid loss 15.54 | valid ppl 5584281.82\n",
      " 140 | time: 57.87s | train loss  0.54 | valid loss 15.67 | valid ppl 6357157.96\n",
      " 141 | time: 57.81s | train loss  0.53 | valid loss 15.70 | valid ppl 6560913.99\n",
      " 142 | time: 57.82s | train loss  0.53 | valid loss 15.68 | valid ppl 6447895.38\n",
      " 143 | time: 57.81s | train loss  0.53 | valid loss 15.83 | valid ppl 7483355.30\n",
      " 144 | time: 57.83s | train loss  0.53 | valid loss 15.65 | valid ppl 6259741.49\n",
      " 145 | time: 57.85s | train loss  0.52 | valid loss 15.79 | valid ppl 7222904.10\n",
      " 146 | time: 57.84s | train loss  0.52 | valid loss 15.57 | valid ppl 5753156.05\n",
      " 147 | time: 57.83s | train loss  0.52 | valid loss 15.83 | valid ppl 7510486.00\n",
      " 148 | time: 57.83s | train loss  0.52 | valid loss 15.79 | valid ppl 7180880.10\n",
      " 149 | time: 57.82s | train loss  0.52 | valid loss 15.69 | valid ppl 6535236.75\n",
      " 150 | time: 57.83s | train loss  0.51 | valid loss 15.96 | valid ppl 8545472.31\n",
      " 151 | time: 57.86s | train loss  0.51 | valid loss 15.73 | valid ppl 6783603.63\n",
      " 152 | time: 57.85s | train loss  0.51 | valid loss 15.88 | valid ppl 7899725.60\n",
      " 153 | time: 57.80s | train loss  0.51 | valid loss 15.83 | valid ppl 7491206.91\n",
      " 154 | time: 57.84s | train loss  0.51 | valid loss 15.74 | valid ppl 6826824.55\n",
      " 155 | time: 57.84s | train loss  0.51 | valid loss 15.80 | valid ppl 7310881.00\n",
      " 156 | time: 57.84s | train loss  0.50 | valid loss 15.92 | valid ppl 8190826.14\n",
      " 157 | time: 57.85s | train loss  0.50 | valid loss 16.00 | valid ppl 8885436.06\n",
      " 158 | time: 57.89s | train loss  0.50 | valid loss 15.71 | valid ppl 6645456.45\n",
      " 159 | time: 57.89s | train loss  0.50 | valid loss 15.99 | valid ppl 8826555.24\n",
      " 160 | time: 57.91s | train loss  0.50 | valid loss 16.02 | valid ppl 9025047.26\n",
      " 161 | time: 57.87s | train loss  0.50 | valid loss 16.04 | valid ppl 9258460.51\n",
      " 162 | time: 57.97s | train loss  0.50 | valid loss 15.98 | valid ppl 8670850.52\n",
      " 163 | time: 57.92s | train loss  0.49 | valid loss 16.00 | valid ppl 8899336.39\n",
      " 164 | time: 57.90s | train loss  0.49 | valid loss 15.95 | valid ppl 8481416.83\n",
      " 165 | time: 57.92s | train loss  0.49 | valid loss 16.02 | valid ppl 9105744.08\n",
      " 166 | time: 57.85s | train loss  0.49 | valid loss 16.24 | valid ppl 11314041.66\n",
      " 167 | time: 57.85s | train loss  0.49 | valid loss 16.08 | valid ppl 9623983.79\n",
      " 168 | time: 57.86s | train loss  0.48 | valid loss 16.05 | valid ppl 9348081.46\n",
      " 169 | time: 57.85s | train loss  0.48 | valid loss 16.15 | valid ppl 10293468.14\n",
      " 170 | time: 57.85s | train loss  0.48 | valid loss 15.91 | valid ppl 8118862.62\n",
      " 171 | time: 57.85s | train loss  0.48 | valid loss 16.08 | valid ppl 9648331.96\n",
      " 172 | time: 57.86s | train loss  0.48 | valid loss 16.28 | valid ppl 11705491.49\n",
      " 173 | time: 57.85s | train loss  0.48 | valid loss 16.28 | valid ppl 11725251.43\n",
      " 174 | time: 57.84s | train loss  0.48 | valid loss 16.20 | valid ppl 10879643.89\n",
      " 175 | time: 57.83s | train loss  0.47 | valid loss 16.19 | valid ppl 10757509.03\n",
      " 176 | time: 57.84s | train loss  0.47 | valid loss 16.23 | valid ppl 11158932.72\n",
      " 177 | time: 57.87s | train loss  0.47 | valid loss 16.25 | valid ppl 11460795.43\n",
      " 178 | time: 57.86s | train loss  0.47 | valid loss 16.23 | valid ppl 11163686.08\n",
      " 179 | time: 57.84s | train loss  0.47 | valid loss 16.11 | valid ppl 9897283.24\n",
      " 180 | time: 57.83s | train loss  0.47 | valid loss 16.34 | valid ppl 12542507.40\n",
      " 181 | time: 57.83s | train loss  0.47 | valid loss 16.32 | valid ppl 12275754.57\n",
      " 182 | time: 57.83s | train loss  0.46 | valid loss 16.27 | valid ppl 11586255.09\n",
      " 183 | time: 57.84s | train loss  0.46 | valid loss 16.11 | valid ppl 9901552.39\n",
      " 184 | time: 57.83s | train loss  0.46 | valid loss 16.28 | valid ppl 11703722.63\n",
      " 185 | time: 57.92s | train loss  0.46 | valid loss 16.44 | valid ppl 13733019.84\n",
      " 186 | time: 57.81s | train loss  0.46 | valid loss 16.25 | valid ppl 11432613.47\n",
      " 187 | time: 57.85s | train loss  0.46 | valid loss 16.18 | valid ppl 10636081.57\n",
      " 188 | time: 57.87s | train loss  0.46 | valid loss 16.29 | valid ppl 11875955.54\n",
      " 189 | time: 57.93s | train loss  0.46 | valid loss 16.25 | valid ppl 11454211.25\n",
      " 190 | time: 57.91s | train loss  0.45 | valid loss 16.23 | valid ppl 11199681.59\n",
      " 191 | time: 57.94s | train loss  0.45 | valid loss 16.39 | valid ppl 13082745.78\n",
      " 192 | time: 57.93s | train loss  0.45 | valid loss 16.23 | valid ppl 11204087.00\n",
      " 193 | time: 57.95s | train loss  0.45 | valid loss 16.43 | valid ppl 13629130.01\n",
      " 194 | time: 57.91s | train loss  0.45 | valid loss 16.25 | valid ppl 11425984.86\n",
      " 195 | time: 57.94s | train loss  0.45 | valid loss 16.44 | valid ppl 13769323.84\n",
      " 196 | time: 57.93s | train loss  0.45 | valid loss 16.26 | valid ppl 11505393.84\n",
      " 197 | time: 57.95s | train loss  0.45 | valid loss 16.40 | valid ppl 13296162.02\n",
      " 198 | time: 57.92s | train loss  0.44 | valid loss 16.29 | valid ppl 11933867.80\n",
      " 199 | time: 57.94s | train loss  0.45 | valid loss 16.62 | valid ppl 16548642.77\n",
      " 200 | time: 57.93s | train loss  0.44 | valid loss 16.41 | valid ppl 13402213.73\n",
      " 201 | time: 57.94s | train loss  0.44 | valid loss 16.50 | valid ppl 14626688.80\n",
      " 202 | time: 57.91s | train loss  0.44 | valid loss 16.47 | valid ppl 14287074.49\n",
      " 203 | time: 57.95s | train loss  0.44 | valid loss 16.46 | valid ppl 14017238.19\n",
      " 204 | time: 57.96s | train loss  0.44 | valid loss 16.55 | valid ppl 15414163.45\n",
      " 205 | time: 57.94s | train loss  0.44 | valid loss 16.58 | valid ppl 15838356.77\n",
      " 206 | time: 57.95s | train loss  0.44 | valid loss 16.55 | valid ppl 15329395.55\n",
      " 207 | time: 57.94s | train loss  0.44 | valid loss 16.54 | valid ppl 15207765.36\n",
      " 208 | time: 57.95s | train loss  0.44 | valid loss 16.47 | valid ppl 14272113.41\n",
      " 209 | time: 57.94s | train loss  0.43 | valid loss 16.56 | valid ppl 15550485.75\n",
      " 210 | time: 57.93s | train loss  0.43 | valid loss 16.55 | valid ppl 15404737.74\n",
      " 211 | time: 57.92s | train loss  0.43 | valid loss 16.54 | valid ppl 15289562.83\n",
      " 212 | time: 57.94s | train loss  0.43 | valid loss 16.39 | valid ppl 13059169.15\n",
      " 213 | time: 57.96s | train loss  0.43 | valid loss 16.38 | valid ppl 12939695.68\n",
      " 214 | time: 57.96s | train loss  0.43 | valid loss 16.54 | valid ppl 15280787.60\n",
      " 215 | time: 57.95s | train loss  0.43 | valid loss 16.68 | valid ppl 17611744.06\n",
      " 216 | time: 57.89s | train loss  0.43 | valid loss 16.51 | valid ppl 14790786.58\n",
      " 217 | time: 57.96s | train loss  0.42 | valid loss 16.70 | valid ppl 17967769.19\n",
      " 218 | time: 57.97s | train loss  0.42 | valid loss 16.64 | valid ppl 16919517.75\n",
      " 219 | time: 57.92s | train loss  0.42 | valid loss 16.59 | valid ppl 16067587.85\n",
      " 220 | time: 57.99s | train loss  0.42 | valid loss 16.66 | valid ppl 17119142.83\n",
      " 221 | time: 57.89s | train loss  0.42 | valid loss 16.62 | valid ppl 16573311.48\n",
      " 222 | time: 57.95s | train loss  0.42 | valid loss 16.60 | valid ppl 16175052.13\n",
      " 223 | time: 57.96s | train loss  0.42 | valid loss 16.61 | valid ppl 16403300.84\n",
      " 224 | time: 57.93s | train loss  0.42 | valid loss 16.77 | valid ppl 19163203.71\n",
      " 225 | time: 57.91s | train loss  0.42 | valid loss 16.61 | valid ppl 16364230.08\n",
      " 226 | time: 57.93s | train loss  0.42 | valid loss 16.73 | valid ppl 18383034.61\n",
      " 227 | time: 57.91s | train loss  0.42 | valid loss 16.77 | valid ppl 19104703.32\n",
      " 228 | time: 57.92s | train loss  0.41 | valid loss 16.68 | valid ppl 17572439.98\n",
      " 229 | time: 57.91s | train loss  0.41 | valid loss 16.86 | valid ppl 20951859.33\n",
      " 230 | time: 57.88s | train loss  0.41 | valid loss 16.83 | valid ppl 20370434.83\n",
      " 231 | time: 57.89s | train loss  0.42 | valid loss 16.67 | valid ppl 17284125.67\n",
      " 232 | time: 57.89s | train loss  0.41 | valid loss 16.57 | valid ppl 15750404.98\n",
      " 233 | time: 57.87s | train loss  0.41 | valid loss 16.85 | valid ppl 20853799.78\n",
      " 234 | time: 57.92s | train loss  0.41 | valid loss 16.85 | valid ppl 20688312.88\n",
      " 235 | time: 57.91s | train loss  0.41 | valid loss 16.81 | valid ppl 19886763.49\n",
      " 236 | time: 57.92s | train loss  0.41 | valid loss 16.79 | valid ppl 19670915.18\n",
      " 237 | time: 57.94s | train loss  0.41 | valid loss 16.64 | valid ppl 16837482.64\n",
      " 238 | time: 57.89s | train loss  0.40 | valid loss 17.11 | valid ppl 26925832.32\n",
      " 239 | time: 57.94s | train loss  0.41 | valid loss 16.76 | valid ppl 18980722.83\n",
      " 240 | time: 57.90s | train loss  0.41 | valid loss 16.88 | valid ppl 21491472.14\n",
      " 241 | time: 57.92s | train loss  0.40 | valid loss 16.79 | valid ppl 19518811.95\n",
      " 242 | time: 57.88s | train loss  0.40 | valid loss 16.83 | valid ppl 20457211.76\n",
      " 243 | time: 57.92s | train loss  0.40 | valid loss 16.89 | valid ppl 21609247.42\n",
      " 244 | time: 57.90s | train loss  0.40 | valid loss 16.83 | valid ppl 20307579.28\n",
      " 245 | time: 57.93s | train loss  0.40 | valid loss 16.92 | valid ppl 22331743.34\n",
      " 246 | time: 57.89s | train loss  0.40 | valid loss 17.02 | valid ppl 24567829.87\n",
      " 247 | time: 57.88s | train loss  0.40 | valid loss 16.87 | valid ppl 21183953.88\n",
      " 248 | time: 57.92s | train loss  0.40 | valid loss 16.94 | valid ppl 22846860.55\n",
      " 249 | time: 57.92s | train loss  0.40 | valid loss 17.02 | valid ppl 24721504.52\n",
      " 250 | time: 57.94s | train loss  0.40 | valid loss 17.07 | valid ppl 25953070.81\n",
      " 251 | time: 57.90s | train loss  0.40 | valid loss 17.03 | valid ppl 24799965.24\n",
      " 252 | time: 57.91s | train loss  0.40 | valid loss 16.96 | valid ppl 23296007.62\n",
      " 253 | time: 57.90s | train loss  0.40 | valid loss 16.96 | valid ppl 23265569.78\n",
      " 254 | time: 57.91s | train loss  0.40 | valid loss 17.02 | valid ppl 24616252.88\n",
      " 255 | time: 57.91s | train loss  0.40 | valid loss 16.86 | valid ppl 21060616.16\n",
      " 256 | time: 57.84s | train loss  0.39 | valid loss 16.87 | valid ppl 21151918.92\n",
      " 257 | time: 57.85s | train loss  0.39 | valid loss 16.88 | valid ppl 21423107.08\n",
      " 258 | time: 57.87s | train loss  0.39 | valid loss 17.16 | valid ppl 28353022.10\n",
      " 259 | time: 57.82s | train loss  0.39 | valid loss 17.08 | valid ppl 26071405.82\n",
      " 260 | time: 57.85s | train loss  0.39 | valid loss 16.90 | valid ppl 21778747.55\n",
      " 261 | time: 57.85s | train loss  0.39 | valid loss 17.06 | valid ppl 25561744.67\n",
      " 262 | time: 57.83s | train loss  0.39 | valid loss 17.05 | valid ppl 25299636.28\n",
      " 263 | time: 57.85s | train loss  0.39 | valid loss 17.04 | valid ppl 25080140.03\n",
      " 264 | time: 57.83s | train loss  0.39 | valid loss 16.98 | valid ppl 23675650.36\n",
      " 265 | time: 57.86s | train loss  0.39 | valid loss 17.06 | valid ppl 25720912.61\n",
      " 266 | time: 57.86s | train loss  0.39 | valid loss 16.95 | valid ppl 23051313.90\n",
      " 267 | time: 57.83s | train loss  0.39 | valid loss 17.10 | valid ppl 26777228.51\n",
      " 268 | time: 57.84s | train loss  0.39 | valid loss 17.15 | valid ppl 28125031.96\n",
      " 269 | time: 57.82s | train loss  0.39 | valid loss 17.09 | valid ppl 26523924.38\n",
      " 270 | time: 57.84s | train loss  0.39 | valid loss 17.07 | valid ppl 25906663.48\n",
      " 271 | time: 57.86s | train loss  0.38 | valid loss 17.15 | valid ppl 28004217.54\n",
      " 272 | time: 57.85s | train loss  0.38 | valid loss 17.13 | valid ppl 27465160.23\n",
      " 273 | time: 57.81s | train loss  0.38 | valid loss 17.11 | valid ppl 26848667.23\n",
      " 274 | time: 57.82s | train loss  0.38 | valid loss 17.10 | valid ppl 26666388.36\n",
      " 275 | time: 57.83s | train loss  0.38 | valid loss 17.23 | valid ppl 30472460.02\n",
      " 276 | time: 57.84s | train loss  0.38 | valid loss 17.05 | valid ppl 25492786.63\n",
      " 277 | time: 57.85s | train loss  0.38 | valid loss 17.19 | valid ppl 29285639.42\n",
      " 278 | time: 57.80s | train loss  0.38 | valid loss 17.04 | valid ppl 25223934.72\n",
      " 279 | time: 57.83s | train loss  0.38 | valid loss 17.29 | valid ppl 32221352.44\n",
      " 280 | time: 57.82s | train loss  0.38 | valid loss 17.20 | valid ppl 29471386.92\n",
      " 281 | time: 57.83s | train loss  0.38 | valid loss 17.16 | valid ppl 28478930.92\n",
      " 282 | time: 57.82s | train loss  0.38 | valid loss 17.14 | valid ppl 27828103.55\n",
      " 283 | time: 57.86s | train loss  0.38 | valid loss 17.13 | valid ppl 27517820.73\n",
      " 284 | time: 57.85s | train loss  0.38 | valid loss 17.13 | valid ppl 27598320.82\n",
      " 285 | time: 57.84s | train loss  0.38 | valid loss 17.12 | valid ppl 27332343.24\n",
      " 286 | time: 57.85s | train loss  0.38 | valid loss 17.35 | valid ppl 34230945.14\n",
      " 287 | time: 57.83s | train loss  0.37 | valid loss 17.24 | valid ppl 30859975.96\n",
      " 288 | time: 57.85s | train loss  0.37 | valid loss 17.26 | valid ppl 31187349.26\n",
      " 289 | time: 57.85s | train loss  0.37 | valid loss 17.33 | valid ppl 33688796.11\n",
      " 290 | time: 57.87s | train loss  0.37 | valid loss 17.14 | valid ppl 27822117.35\n",
      " 291 | time: 57.85s | train loss  0.38 | valid loss 17.29 | valid ppl 32192014.27\n",
      " 292 | time: 57.79s | train loss  0.37 | valid loss 17.23 | valid ppl 30535289.83\n",
      " 293 | time: 57.83s | train loss  0.37 | valid loss 16.98 | valid ppl 23708583.55\n",
      " 294 | time: 57.84s | train loss  0.37 | valid loss 17.11 | valid ppl 26914786.62\n",
      " 295 | time: 57.84s | train loss  0.37 | valid loss 17.19 | valid ppl 29127831.64\n",
      " 296 | time: 57.80s | train loss  0.37 | valid loss 17.24 | valid ppl 30683853.18\n",
      " 297 | time: 57.85s | train loss  0.37 | valid loss 17.34 | valid ppl 33869142.40\n",
      " 298 | time: 57.84s | train loss  0.37 | valid loss 17.51 | valid ppl 40111509.57\n",
      " 299 | time: 57.83s | train loss  0.37 | valid loss 17.31 | valid ppl 32783073.43\n",
      " 300 | time: 57.81s | train loss  0.37 | valid loss 17.48 | valid ppl 39059030.19\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Loop over epochs. Adjust the learning rate after each epoch.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "epochs    = 300\n",
    "\n",
    "store_models = True\n",
    "name = f'wikitext103-transformer-prc:{prc}'\n",
    "models_dir = '/gstore/scratch/u/loukasa1/alg-stability' \n",
    "store_models_interval = 1\n",
    "\n",
    "model_dir = f'{models_dir}/model_weights/{name}_{datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\")}'\n",
    "if not os.path.exists(model_dir): os.makedirs(model_dir)\n",
    "print(model_dir)\n",
    "\n",
    "losses_train, losses_val = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    losses_epoch = train(model)\n",
    "    losses_train.extend(losses_epoch)\n",
    "    \n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl  = math.exp(val_loss)\n",
    "    losses_val.append(val_loss)\n",
    "    \n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print(f' {epoch:3d} | time: {elapsed:5.2f}s | train loss {np.mean(losses_epoch):5.2f} | ' + \n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "\n",
    "    if epoch < 10: scheduler.step()\n",
    "        \n",
    "    if store_models and epoch % store_models_interval == 0:\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'transformer_encoder.layers.0.linear1.weight': model.state_dict()['transformer_encoder.layers.0.linear1.weight'], \n",
    "                    'loss': np.mean(losses_epoch), \n",
    "                    'loss_val': val_loss, \n",
    "                    'loss_test': evaluate(model, test_data)}, model_dir + f'/state_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the last model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 17.50 | test ppl 39918036.16\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fff3c00b550>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7, 6), facecolor=(1,1,1))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(np.linspace(1, epoch, len(losses_train)), losses_train)\n",
    "ax.plot(losses_val, 'r-')\n",
    "# ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
